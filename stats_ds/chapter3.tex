\graphicspath{%
{chapter3graph/}%
{chapter3graph/bg/}}
%\makeindex

\chapter{Testing}

\section{Z test}
\label{ztest}

A Z-test is any statistical test for which the distribution of the test statistics (i.e. a quantity derived from the sample) under the Null Hypothesis can be approximated by a normal distribution. Z-test tests the mean of a distribution. \\

How to perform a Z test when $T$ is a statistics that is approximately normally distributed under the Null Hypothesis:\\
1) estimate the expected value (mean) $\mu$ of $T$ under the Null Hypothesis, and obtain the standard deviation $\sigma$ of T.\\
2) determine the properties of $T$: one tailed or two tailed: \\
For Null Hypothesis $H_0: \mu \geq \mu_0$ vs Alternative Hypothesis $H_1: \mu < \mu_0$, it is right one-tailed test.\\ 
For Null Hypothesis $H_0: \mu \leq \mu_0$ vs Alternative Hypothesis $H_1: \mu > \mu_0$, it is left one-tailed test.\\ 
For Null Hypothesis $H_0: \mu = \mu_0$ vs Alternative Hypothesis $H_1: \mu \neq \mu_0$, it is two-tailed test.\\ 
where $\mu$ is the true mean of the population under analysis, $\mu_0$ is the hypothesised mean of the population under analysis. \\

3) calculate the Z score according to Eqn.(\ref{zscorenormv1}), then the one-tailed or two-tailed p-value can be obtained. This p-value is then compared with the desired confidence level to determine if $H_0$ is rejected or not.

In summary, a simple case of Z test is that we have a population distribution (with a certain population mean and standard deviation). Now given a new sample, we would like to know if the mean of the new sample is significantly differ from the population mean, and we can use z test to find out assuming the distribution follow normal distribution.\\

\underline{Example 1}\\
In a region (population) the mean and standard deviation of scores on a reading test is 100 and 12. Our interest (sample) is in the score of 55 students in a particular class who received a mean score of 96. We can ask whether this mean score is significantly lower than the region mean, that is, are the students in this school comparable to a simple random sample of 55 students from the region as a whole, or are their scores surprisingly low ?\\

Using Eqn.(\ref{zscorenormv1}), we can calculate the Z score as:
\begin{eqnarray}
Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} = \frac{96-100}{12/\sqrt{55}} = -2.47
\end{eqnarray}
In this example we treat the population mean and variance as known, which would be appropriate if all students in the region were tested. When population parameters are unknown, a \underline{t test} should be conducted instead.\\

The classroom mean score is 96, which is $-2.47$ standard error (since we divided by $\sqrt{n}$, we have converted it into standard error) from the population mean of 100. Looking up the z-score in the table of standard normal distribution cumulative probability, we find that the probability of observing a standard normal value below $-2.47$ is around $0.5 - 0.4932 = 0.0068$. This is the one-tailed p-value for the Null Hypothesis that the 55 students are comparable to a simple random sample from the population of all test-takers. The Z test tells us that the 55 students of interest have an unusually low mean test score compared to the population mean. \\

\underline{Example 2}\\

A complaint was registered stating that the boys in the school were underfed. Average weight of boys of age 10 is 32kg with 9kg standard deviation (population). a sample of 25 boys was selected from the school and the average is found to be 29.5kg. At 0.05 significance level ($\alpha = 0.05$), we need to check whether the complaint is true or not.\\

The Null Hypothesis says that there is no significant difference between the boys and the whole population. i.e.\\
$H_0:\mu = 32$, no significant difference between the students.\\
The Alternate Hypothesis says that there is a significant difference between the boys and the whole population and the complaint is true, i.e.:\\
$H_1:\mu < 32$, there is a significant difference between the students.\\


Using the Eqn. (\ref{zscorenormv1}), we have Z value of $\frac{29.5-32}{9/\sqrt{25}} = -1.39$. This Z value can be used to find the p-value of the same selected samples. The corresponding p-value is 0.0823. This p-value is greater than $\alpha=0.05$, this means that in this left-tailed test, we can accept (cannot reject) the Null Hypothesis. This means that there is a probability of 0.0823 that a sample from the population can have more than 1.39 standard error from the population mean, as demonstrated by our sample (because we want to know if a sample drawn from this population is the same as the sample we are studying.), and it is larger than our $\alpha$. In this case, there is no difference between the students (at this confidence level) and the complaint is not true.

\section{T test}
\label{ttest}

T test are very similar to Z test described above. Both test usually requires the sample means to exhibit normality for exactness. In addition, whether using T test or Z test depends on:\\

1) The size of our sample. The magic number is usually around 30-50. Below that is considered a small sample. When the sample size is large enough, the central limit theorem kicks in and we do not need to worry too much about the population is normally distributed. \\

2) If we know the population standard deviation ($\sigma$). In real life we usually do not know this value. But in certain cases we can have this information. \\

If sample size is large and $\sigma$ is known, we can use Z test. If sample size is large and $\sigma$ is unknown, use T test. If the population is small, usually use T test instead unless population $\sigma$ is known. The formula is similar to Z-statistics:

\begin{eqnarray}
t-statistics = \frac{\bar{X} - \mu}{S/\sqrt{n}}
\end{eqnarray}

where $\mu$ is the population mean and $S$ is the sample standard deviations. 



\section{Chi-squared test}
A chi-squared test ($\chi^2$ test) is a statistical hypothesis test that is valid to perform when the test statistic is chi-squared distributed under the Null Hypothesis, specifically Pearson's chi-squared test. The Pearson's chi-square test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories.\\

Suppose that $n$ observations in a random sample from a population are classified into $k$ mutually exclusive classes with respective observed numbers $x_i$ (for i = 1, 2, ... , k) and a Null Hypothesis gives the probability $p_i$ that an observations falls into the $i$th class. So we have expected numbers $m_i = np_i $.\\

The chi-square statistics:
\begin{eqnarray}
X^2 = \sum_{i=1}^{k} \frac{(x_i - m_i)^2}{m_i}
\end{eqnarray}
The expected number $m_i$ is large enough known numbers in all cells assuming every $x_i$ maybe taken as normally distributed. And then in the limit that $n$ becomes large, $X^2$ follows the $\chi^2$ distribution with $k-1$ degrees of freedom. We can then look up the $\chi^2$ table to obtain (one- or two-tail) p-value to determine if Null hypothesis should be rejected. \\

\underline{Example}\\

For example, we want to investigate what is the distribution of the number of customers we get each day for a particular restaurant ? The restaurant owner claimed that the distribution is the expected number in the table below. We would like to see how good this distribution he claimed actually fits the observed data. \\

\begin{table}[h!]
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Day         & M  & T  & W  & T  & F  & S  \\ \hline
Observed    & 30 & 14 & 34 & 45 & 57 & 20 \\ \hline
Expected    & 20 & 20 & 30 & 40 & 60 & 30 \\ \hline
Expected \% & 10 & 10 & 15 & 20 & 30 & 15 \\ \hline
\end{tabular}
\end{table}

We can make the Null Hypothesis that the owner's distribution is correct, i.e. $H_0$: The expected number of customer is right. Then $H_1$: The expected number is wrong. And I want to do this with a significance level of $5\%$ or $\alpha = 0.05$. We are going to calculate a test statistics here based on the table, this statistics is the chi-square statistics.  Another way to understand it is that the statistics we are calculating has approximately a chi-square distribution. And given that it does have a chi-square distribution with a certain number of degrees of freedom, we would like to see the probability of getting the observed results (or more extreme) is less than $5\%$, in that case we can reject the Null Hypothesis. \\

From the observed data, we obtained that the total number of customers is 200, which can be used to calculate the Expected number of customers in the table above. Then using the chi-square formula, we can obtain the chi-square statistics (approximate to $\chi^2$):
\begin{eqnarray}
X^2 &=& \frac{(30-20)^2}{20} + \frac{(14-20)^2}{20} + \frac{(34-30)^2}{30} \\ \nonumber
       &+&\frac{(45-40)^2}{40} + \frac{(57-60)^2}{60} + \frac{(20-30)^2}{30} \\ \nonumber
       &=& 11.44
\end{eqnarray}

If we assume this test statistics follows a chi-square distribution, what is the probability of getting a result this extreme ? For our particular case, the degree of freedom is $(6-1)=5$. If we check out a chi-square table of the graph of chi-square PDF (Fig.(\ref{cspdf})), the value for 11.44 is a bit less than $5\%$. Hence we can reject $H_0$.

\section{A/B testing}
\label{abtest}

A/B testing (also known as bucket testing or split-run testing) is a user experience research methodology. A/B tests consist of a randomised experiment with with two variants, A and B. It includes application of \underline{statistical hypothesis testing} (Null vs Alternative hypothesis) or two-sample hypothesis testing. \\


\subsection{Null hypothesis $H_0$}

The Null hypothesis usually states that there is no difference between groups in study. For example: there is no relationship between the risk factor (or treatment) being studied AND the occurrence of the health outcome in a medical study. (In this case, Group A is a placebo group and Group B receiving a new drugs).

\subsection{Alternative hypothesis $H_1$}

The Alternative hypothesis states that there is a difference between groups in study. There IS a relationship between the treatment or risk factor AND the outcomes of the experiment.\\

By default, we assume that the Null Hypothesis is true, until we have enough evidence to support rejecting this hypothesis. A bummer if Null Hypothesis is True. \\

But we can NEVER prove the Alternative Hypothesis is true, the best we can do is to REJECT a hypothesis (saying it is false) or FAIL to reject a hypothesis (could be true, but never sure). So usually we want to reject the Null Hypothesis, because that is the as close as we can get to prove the Alternative Hypothesis.

\subsection{Type I/II error}

Rejecting the Null Hypothesis when $H_0$ is True is called Type I error (also known as False Positive), i.e. the researcher say there is a difference between the groups when there really isn't. This error is usually the focus because researcher wants to show $H_0$ is False. The probability of making a Type I error is called $\alpha$, 

Type II error (also known as False Negative) is when we failed to reject $H_0$ when we should reject it. The probability of making a Type II error is called $\beta$. 

\subsection{Statistical Power}

Power = probability of finding a difference between groups if one truly exist. (i.e. $H_0$ is False)\\
= probability of not making a Type II Error \\
= $1 - \beta$\\

A good power is around 0.8. Power matters during experiment design. We should do power calculations based on projections.\\

Power increases when:\\
1) increase in sample size. i.e. you have more data to make a conclusion.\\
2) (big) actual difference between groups, i.e. effect size\\
3) (good) precision of results, i.e. multiple samples show consistent results instead of all over the place.\\

\subsection{Test of Significance}

A common indicator when testing significance is the p-value. \\

p-value is the probability of obtaining a sample more extreme than the ones observed in our data, assuming $H_0$ is True. If this value is low, then it means either our power is low or there is a low probability of observing this value if the Null Hypothesis is True. This represents a measure of evidence against retaining $H_0$. We don't have to prove $H_0$ to use this, we just assume it is true before using the p-value.\\

For example, we can calculate z-test value for a right tailed test assuming normal distribution. Then the p-value is just the area under the curve to the right of the z-test score.   \\

What determines if p-value is low or high ?\\
We use $\alpha$, which is also called level of significance. It is a selected cut-off point to determine if the p-value is acceptably high or low.\\

We define our probability of not making type I error as the confidence level, a common value for this is 0.95.\\

Standard p-value:\\
<0.01: very strong evidence against the Null hypothesis.\\
0.01 - 0.05: strong evidence against the Null hypothesis.\\
0.05 - 0.10: very weak evidence against the Null hypothesis. \\
more than 0.1: small to no evidence against the Null hypothesis. \\


\section{Hypothesis testing (one-way and two-way)}

\section{ANOVA}

\section{ANCOVA}

\section{One-sample/Two-sample bootstrap hypothesis test}

\section{Time series: p, d, q parameters, unit root and box test}



